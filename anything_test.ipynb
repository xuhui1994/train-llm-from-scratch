{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Mask:\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_causal_mask(seq_length: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    创建因果掩码（Causal Mask）\n",
    "    参数:\n",
    "        seq_length: 序列长度\n",
    "    返回:\n",
    "        mask: 形状为 (seq_length, seq_length) 的掩码张量\n",
    "             0 表示允许注意力，1 表示屏蔽注意力\n",
    "    \"\"\"\n",
    "    # 创建上三角矩阵（不包含对角线）\n",
    "    mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)\n",
    "    # 将掩码转换为布尔类型\n",
    "    mask = mask.bool()\n",
    "    return mask\n",
    "\n",
    "# 测试代码\n",
    "seq_len = 4\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "print(\"Causal Mask:\")\n",
    "print(causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: tensor([[[ 1.5958, -1.4635, -1.4237,  1.2843],\n",
      "         [ 1.7220,  0.9283, -0.4925,  0.3239],\n",
      "         [ 0.8589,  0.9786, -0.0241, -0.1395],\n",
      "         [ 0.6889, -0.1886,  0.2087, -1.0112]],\n",
      "\n",
      "        [[-2.2927,  0.8763, -1.5735, -0.2772],\n",
      "         [-0.9213, -0.9578,  1.8836,  2.6790],\n",
      "         [ 1.2034,  0.4874, -0.8374, -1.0190],\n",
      "         [-0.1216,  0.7642, -1.0607, -1.5804]]])\n",
      "scores: tensor([[[ 1.5958,    -inf,    -inf,    -inf],\n",
      "         [ 1.7220,  0.9283,    -inf,    -inf],\n",
      "         [ 0.8589,  0.9786, -0.0241,    -inf],\n",
      "         [ 0.6889, -0.1886,  0.2087, -1.0112]],\n",
      "\n",
      "        [[-2.2927,    -inf,    -inf,    -inf],\n",
      "         [-0.9213, -0.9578,    -inf,    -inf],\n",
      "         [ 1.2034,  0.4874, -0.8374,    -inf],\n",
      "         [-0.1216,  0.7642, -1.0607, -1.5804]]])\n",
      "attention_weights: tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6886, 0.3114, 0.0000, 0.0000],\n",
      "         [0.3936, 0.4436, 0.1628, 0.0000],\n",
      "         [0.4510, 0.1876, 0.2790, 0.0824]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5091, 0.4909, 0.0000, 0.0000],\n",
      "         [0.6178, 0.3019, 0.0803, 0.0000],\n",
      "         [0.2470, 0.5990, 0.0966, 0.0574]]])\n",
      "输入形状: torch.Size([2, 4, 8])\n",
      "输出形状: torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def apply_causal_mask(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    在自注意力计算中应用因果掩码\n",
    "    参数:\n",
    "        query: 查询张量 shape (batch_size, seq_len, d_model)\n",
    "        key: 键张量 shape (batch_size, seq_len, d_model)\n",
    "        value: 值张量 shape (batch_size, seq_len, d_model)\n",
    "    返回:\n",
    "        attention_output: 注意力输出\n",
    "    \"\"\"\n",
    "    # 计算注意力分数\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))  # (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # 缩放注意力分数\n",
    "    d_k = query.size(-1)\n",
    "    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    print(\"scores:\", scores)\n",
    "    \n",
    "    # 创建并应用因果掩码\n",
    "    seq_len = query.size(1)\n",
    "    mask = create_causal_mask(seq_len)\n",
    "    \n",
    "    # 将掩码位置的值设置为一个很大的负数\n",
    "    scores = scores.masked_fill(mask, float('-inf'))\n",
    "    print(\"scores:\", scores)\n",
    "    \n",
    "    # 应用softmax获取注意力权重\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    print(\"attention_weights:\", attention_weights)\n",
    "    \n",
    "    # 计算输出\n",
    "    attention_output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return attention_output\n",
    "\n",
    "# 测试代码\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "# 创建示例输入\n",
    "query = torch.randn(batch_size, seq_len, d_model)\n",
    "key = torch.randn(batch_size, seq_len, d_model)\n",
    "value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# 应用因果掩码的注意力计算\n",
    "output = apply_causal_mask(query, key, value)\n",
    "print(\"输入形状:\", query.shape)\n",
    "print(\"输出形状:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [False  True  True]\n",
      " [False False  True]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "n = 3\n",
    "mask = np.ones((n, n), dtype=bool)\n",
    "mask = np.triu(mask,k=0)\n",
    "\n",
    "print(mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
